用来记载自己做项目时的细节，防止遗忘，基本上本文档会最终整合到其他文档中，读者没必要看，只是自己逐步实现项目时的记录，当然如果你感兴趣一步步怎么搭建出来的可以看看，作为一个可选项吧。
# 日志系统
> made in 2021.11.27

* GDB多线程调试真麻烦，虽然很清晰，但是我始终觉得效率不高，还是直接printf实在，不过gdb用来学习代码逻辑倒是非常不错。
* 有一说一，智能指针真的很好用，能帮你发现许多写代码时的小问题，除了什么RAII、copy on write、对象生命期管理等一堆手法技巧，还能帮你检查代码逻辑呢，~~具体原因可以看我上面的第一条~~push_back()时没用右值引用的bug自动检查。

# 初步Reactor
**Channel,EventLoop,Poller**
这三个构成了reactor的核心框架
channel为具体事件分发器，每一个文件描述符都对应一个channel，每一个Poller都拥有所有的已经注册的Channel，并通过map管理。
EventLoop 拥有唯一的一个Poller对象，借助它实现I/O复用。每次EventLoop调用poll，Poller将激活的Channels列表提供给EventLoop,
相当于Poller是其的一个工具类。
EventLoop拥有所有channel，并且通过成员函数可以自己选择是否注册channel到自己的poller。
这种设计分层方便替换I/O复用工具，比如把Poller换成Epoll。

细节：
1. channel定义时指定自己的eventloop，channel.enableReading()时注册Poller
2. 用了大量断言，确保每个channel 和 poller的使用时是在所属于的rector线程(eventloop线程)。增加程序健壮性。

# 定时器TimerQueue

**Timer类**:具体的定时器事件，具有几大属性，即定时时间、重复性与否、周期间隔时间，以及具体绑定的回调函数

```c++
Timestamp expiration_;
const bool repeat_;
const double interval_;
const TimerCallback callback_;
```
**TimerQueue类**:

* 定时器的管理着，处理定时器集合，定时到期处理，设置下一个定时时间等，
拥有实际的timefd文件描述符，在初始化时便将文件描述符插入了Poller可读，一但设置定时Timer则开始运转。

* 为具体读timefd的对象（timefd每次触发一定要读出来，因为采用的水平触发），读完timefd后计算当前时间，计算
此时到期时间，并使用管理的timefd的回调函数逐个处理对应的注册回调，然后删除过期时间（会同时计算repeat Timer重新插入），重新取最新的定时Timer为第一。
* 采用红黑树set管理定时器，因为红黑树支持快速插入，自动排序等功能，不采用map是因为已经使用了pair<,>。
  
* 每一个I/O线程拥有唯一的一个TimerQueue，所以是针对每个reactor的定时。

* 用户不能直接使用它的成员函数插入，对用户不可见。用户使用外部EventLoop类的接口进行插入定时器。但这样会存在线程安全性问题，因为有可能多个线程设计此I/O线程的定时，因此使用EventLoop::runInLoop()函数实现事件转发，让该事件转发给I/O线程来做

**EventLoop类**： 更新方法

*一系列解决reacotrs通信线程安全的方法*

EventLoop用容器vector提供回调队列，各回调函数若因为线程安全问题无法运行时，会将函数回调插入此队列，
I/O线程会在执行完所有channel回调后，再执行完队列回调。**巧妙的解决了线程安全问题，将事件转发给实际解决回调函数的地方。

注意：每次插入了回调函数，应该唤醒其对应的I/O线程(一般阻塞于poll)，具体实现方法时注册一个eventfd,取名为wakefd,插了回调则在wakefd里面写一个字节就行。值得一提的是，因为wakefd单单就是起到唤醒I/O线程的作用，所以他对应的channel实际上除了刷新这个fd什么都没做，因为他要做的事情都放在回调队列里面去了。

如下的`doPendingFunctors()`位置:
```c++
void EventLoop::loop()
{
/**/
  while (!quit_)
  {
    activeChannels_.clear();
    pollReturnTime_ = poller_->poll(kPollTimeMs, &activeChannels_);
    for (ChannelList::iterator it = activeChannels_.begin();
        it != activeChannels_.end(); ++it)
    {
      (*it)->handleEvent();
    }
    doPendingFunctors();
  }
}
```

EventLoop::runInLoop()方法：
当前线程是I/O线程直接运行回调函数，若不是，插入EventLoop的回调队列
```c++
void EventLoop::runInLoop(const Functor& cb)
{
  if (isInLoopThread()) {
    cb();
  }
  else {
    queueInLoop(cb);
  }
}
```
EventLoop::queueInLoop()方法:
所以真正有线程安全问题的地方只有pendingFunctors_,加锁就好了
```c++
void EventLoop::queueInLoop(const Functor& cb)
{
  {
  MutexLockGuard lock(mutex_);
  pendingFunctors_.push_back(cb);
  }

  if (!isInLoopThread() || callingPendingFunctors_) {
    wakeup();
  }
}
```

并且在执行回调队列的时候采用了交换手法将执行回调函数放在了临界区外,最大限度减少race condition
```c++

void EventLoop::doPendingFunctors()
{
  std::vector<Functor> functors;
  callingPendingFunctors_ = true;
  {
  MutexLockGuard lock(mutex_);
  functors.swap(pendingFunctors_);
  }

  for (size_t i = 0; i < functors.size(); ++i) {
    functors[i]();
  }
  callingPendingFunctors_ = false;
}
```
这也是reactor模型的巧妙之处，one loop per thread!!!

插叙：成员变量初始化列表的初始化顺序只与你定义时候的顺序有关，和你初始化列表写的顺序没关系！这个BUG改了为一小时，就因为初始化线程ID放在了后面

再次实名吐槽多线程BUG太难改了。因为多写了一个assertRunInLoop又改了我bug一小时，日志基本上看不出来，只能GDB一步步跑。
多线程程序一定要构思好，毕竟最好的设计就是一开始就想清楚的设计。

**EventLoopThread类**： 

自带一个EventLoop,每声明一个此类对象就相当于创建了一个新的I/O线程(subReactor)，其开始方法startLoop会返回他的EventLoop指针，
方便主线程进一步操作。
```c++
void runInThread()
{
  printf("runInThread(): pid = %d, tid = %d\n",
         getpid(), CurrentThread::tid());
}

int main()
{
  printf("main(): pid = %d, tid = %d\n",
         getpid(), CurrentThread::tid());

  EventLoopThread loopThread;
  EventLoop* loop = loopThread.startLoop();
  loop->runInLoop(runInThread);
  sleep(1);
  loop->runAfter(2, runInThread);
  sleep(3);
  loop->quit();

  printf("exit main().\n");
}
```
以上例子全程是主线程在向subReactor增加时间事件，就是用了之前提到的回调函数线程转移的手法。

**core dump 文件**

core文件产生路径也需要改 sudo bash -c 'echo core > /proc/sys/kernel/core_pattern 

core文件调试 gdd ./test1 core,则可以i threads ->> bt函数栈打开 看core dump 前的数据了
core文件改大小 需要改 sudo vim /etc/security/limits.conf 

最后一行加 hjg hare core unlimited
         hjg soft core unlimited

然后重启终端执行 ulimit -c unlimited
        
服务器eventLoop开启循环时会直接打开以下fd,并注册到Poller:

AcceptorFd TimeQueueFd WakeupFd

logFileFd也有，但是不由Poller监听

**tips**:

经过测试发现，如果接受了一个套接字连接但是却不保存这个套接字，这个fd会一直存在，但是对于服务器来说却丢失了套接字的管理，即使对方关闭了连接，你这边的连接依然关不掉，
通过学到的网络知识，猜测是服务器这边的套接字进入了CLOSE_WAIT状态，只有程序员显示close或shutdown(write)才能将最终的FIN发出去。
```
//如日志：connfd会一直增加，只有自己close才会减少
2021-11-29 17:05:26 TRACE 194803  new connection from : 127.0.0.1:48176现在的connfd:7 -- Acceptor.cc:37
2021-11-29 17:05:26 DEBUG 194803  Socket::accept -- SocketsOps.cc:78
2021-11-29 17:05:31 TRACE 194803  1 events happended -- Poller.cc:26
2021-11-29 17:05:31 TRACE 194803  new connection from : 127.0.0.1:48178现在的connfd:8 -- Acceptor.cc:37
2021-11-29 17:05:31 DEBUG 194803  Socket::accept -- SocketsOps.cc:78
2021-11-29 17:05:41 TRACE 194803  1 events happended -- Poller.cc:26
2021-11-29 17:05:41 TRACE 194803  new connection from : 127.0.0.1:48182现在的connfd:9 -- Acceptor.cc:37
2021-11-29 17:05:41 DEBUG 194803  Socket::accept -- SocketsOps.cc:78
2021-11-29 17:05:51 TRACE 194803   nothing happended -- Poller.cc:29
2021-11-29 17:07:13 TRACE 194803  new connection from : 127.0.0.1:48202现在的connfd:10 -- Acceptor.cc:37
```

**linux命令**

查看线程 : 
```
ps -aux | grep ./test1
ps -T -p (上面的进程号)
```

**Acceptor类**

作为主reactor的监听套接字，每一个可以监听一个端口，可将其分发给不同的eventLoop(从reactor)

修改了muduo的长连接逻辑，陈硕认为不应该一次读完accept队列，并且会多读一次增加一次系统调用，但我看相关资料，这样对瞬时短连接会有
很大阻碍，考虑到muduo完全倾向于长链接，而大幅度放弃了短连接的并发，所以我改成了while读完accpet队列，直到读到EAGAIN。

限制并发连接数，解决文件描述符不足或者超负载问题：

```c++
void Acceptor::handleRead() {
    InetAddress peerAddr(0);
    int connfd = 0;
    while ( (connfd = acceptSocket_.accept(&peerAddr)) > 0) {
        LOG_TRACE<<"new connection from : "<<peerAddr.toHostPort() <<" 现在的connfd:"<<connfd;
        //kMaxConnections实际上是文件描述符数，不过系统常fd一定，所以可以看做最大并发连接数
        if (connfd < kMaxConnections) {
            if (newConnectionCallback_) {
                newConnectionCallback_(connfd, peerAddr);
        }
        else {
            sockets::close(connfd);
        }
        }
    }

}
```

**连接的层层转发：**

TcpServer.start()后，在该对应的eventloop里激活Acceptor

Acceptor收到新连接，回调调用TcpServer::newConnection,在其中建立已经连接的套接字的TcpConnection
，并转发连接套接字、sockaddr、用户设置的回调给这个新建立的TcpConnection，所以TcpConnection建立时已经连接好了。

最后执行TcpConnection::connectEstablished，将连接套接字可读事件注册给了Poller,然后所有连接完毕，执行用户注册的
ConnecitonCallback。

TcpConnection是连接的实际管理类，回调函数都在此类的函数中执行。

连接套接字可读时，TcpConnetion的Channel会调用用户注册的回调MessageCallback，所以执行Callback的实体始终是对应的TcpConnetion对象。

~~因为TcpConnetion生命期的模糊，为了正确析构连接，必须继承enable_shared_from_this,每次操作连接都用新的智能指针控制。~~（后文有详细解释）


tips:

测试时水平触发的一个问题，如果对面套接字关闭了，你读这个套接字返回0，并且如果你这次不赶紧处理close掉，下次还会继续触发，相当于永远busy loop.

目前测试POLLHUP和POLLRDHUP触发条件一致，都是对方关闭，谷歌上的说法没看明白，姑且先当成一样。所以POLLHUP事件一定要在前面处理好，调用closeCallback_。

**Callback.h**

外界可见可设置的回调，用户可以在此设置自己的回调，其他回调都是内部回调，对外界透明。

**muduo一个重要的BUG**

我发现muduo的TcpConnection的Channel和普通的Channel一样，没有注册POLLRDHUP事件，因此根本没法响应对面连接关闭的事件，至少我测试起来是这样。

值得一提的是，连接关闭时，POLLRDHUP和POLLIN是同时置位的。

作为修改，我在TcpConnection对象创立时加上我关心的事件，测试后通过。

但这样就有了把私有成员events_暴露出来的风险，作为代价，不过没有影响。

！更新，其实muduo这个bug好像可以通过read返回0解决，把逻辑前移到了handleRead，否则channel调用closeCallBack反而让程序后续改起来不方便了。
前移让自己调用closeCallBack前还能加自己Tcp的独特逻辑，毕竟closeCallBack是用户提供的。这样还有一个好处就是只用了POLLIN信号，之后换EPOLL更加方便了，减少了程序的耦合性。
```c++
TcpConnection::TcpConnection(...)
{
..........
  channel_->events() = POLLRDHUP | POLLERR;
  channel_->setReadCallback(
      bind(&TcpConnection::handleRead, this));
..........
}

int& events() { return events_; }
```

水平触发有个细节要主要，如果POLLRDHUP没有及时关闭套接字，日志会一直写，一次把我电脑写了几百万行直接死机。发现日志保护措施在高速CPU面前还是很局限。

处理TcpConnection关闭的逻辑很有趣，先只是简单的关闭Poller的关注事件(events_置0),为了防止在处理时关闭（延长对象生命期），closeCallBack调用的shared_fron_this产生的
智能指针，因此会在智能指针析构时才closefd且dtor对象。

close时，先调用TcpConnection的channel对应的回调handleClose().接着会回调TcpConnection自身附带回调closeCallback_,其会调用TcpServer的removeConnection(),
将其从TcpServer的字典划去后，再最终调用TcpConnection的connectDestroyed().

**关闭TcpConnection对应的疑难问题**:

个人觉得，这应该是整个程序逻辑最复杂，细节最多的地方，也是多线程中对RAII的难点的挑战和攻破。

一步一步来说明其中的细节的难点，到底什么是对象生命期管理，又什么是延长对象生命期以确保正确析构。

```c++
void TcpServer::newConnection(int sockfd, const InetAddress& peerAddr) {
..........
    TcpConnectionPtr conn(
        new TcpConnection(loop_, connName, sockfd, localAddr, peerAddr));
    connections_[connName] = conn;

    conn->setCloseCallback( bind(&TcpServer::removeConnection, this, _1));
..........
}
```
注意TcpConnectionPtr的建立，是在newConnection()函数的栈上创建，因为将其放入了map字典里，所以目前TcpConnection不会早于字典里被erase前析构。

接着触发了handleClose()事件，会回调TcpConnection自身的closeCallback_,其对应于TcpServer::removeConnection()
```c++
void TcpConnection::handleClose() {
......
    closeCallback_(shared_from_this());
......
}
```
注意以上用了shared_from_this(),相当于又创建了一个当前TcpConnection的shared_ptr，引用计数增加为2

接着函数内部如下

```c++
void TcpServer::removeConnection(const TcpConnectionPtr& conn) {
.......
    size_t n = connections_.erase(conn->name());

    loop_->queueInLoop(
        bind(&TcpConnection::connectDestroyed, conn));
.......
}
```

可以发见，在函数内部删去了字典里的智能指针，所以引用计数重回1，因此如果前面没有shared_from_this()，则TcpConnection对象死亡，对象生命期异常，程序崩溃。

上面还有一个细节，为了不在Poller事件其中处理对象析构，将connectDestroy析构放到了后续处理回调函数队列的位置。其中用了bind()继续延长其生命期到了执行回调队列中的connectDestroyed()时。

最后在connectDestroyed()中完成最后的处理，如从EventLoop中移除对应的channel，如下代码：
```c++
void TcpConnection::connectDestroyed()
{
.....
    loop_->removeChannel(get_pointer(channel_));
.....
}
```
EventLoop会转发给他的实际处理I/O复用的成员Poller处理,Poller会正确维护其Channel列表

removeChannel()执行完后，最终TcpConnection生命期终结，因为其拥有他对应的所有成员的指针,如unique_ptr<Channel>，则该TcpConnection对象对应的所有数据成员均正确析构。
close(fd)在unique_ptr<Sockets>析构时执行。

所以一切析构，资源最终的释放都在TcpConnection的析构函数中，生命期管理感觉本质上就是一种滞后析构，防止执行时却在析构。

**面向对象编程更深的理解**

经过项目的不断深入，对面向对象模型与封装理解更深。
每个对象的定义应当尽量明确，发生了什么事件，一定要交给此事件处理的类来处理，不能事件处理的类模棱两可。

举个例子，实际上TcpConnection有他更上层对象的指针，比如EventLoop,或者更上层TcpServer,理论上他们都可以处理实际连接事件，
但是在项目中，都会将事件逐层转发到正确处理这个事件的地方，比如用户注册的回调转发给TcpConnection的内部回调函数。

比如最底层的实际处理的对象都是Channel，channel转发给实际的事件处理器(其上层)，如TimeQueue的handleRead(),如wakefd的
handleRead(),抑或是TcpConnection的handleRead()。就像我前文写的一样TcpConnection的handleRead()还会依次的转发到实际处理的地方，完成事件处理。

比如析构时转发给TcpServer删除其字典，转发给EventLoop的Poller删除Poller监视，将TcpConnection的析构分层甚至分时的处理，一级一级的将TcpConnection对象完全的析构掉。

**一个不可思议的转换**

读muduo源码发现了一个bind funciton转换，百思不得其解。
```c++
//frist step
acceptChannel_.setReadCallback( bind(&Acceptor::handleRead, this) );
//对应的hanleRead声明
void Acceptor::handleRead()
//由上见，bind应该是产生了一个 void()的可调用对象，但是不可思议的是
typedef function<void(Timestamp)> ReadEventCallback;
void setReadCallback(const ReadEventCallback& cb)
{ readCallback_ = cb; }
//setReadCall函数参数竟然是void(Timestamp)类型的函数对象，这其中发生了转换？
```
进一步我自己写了个测试
```c++
class A {
public:
    A(int a) {
        a_ = a;
    }
private:
    int a_;
};
typedef function<void(A)> voidACallback;

void fk(const voidACallback& h) {
    h( A(3)); 
}
void print() {
    cout<<"it's ok.";
}

int main() {
    voidACallback h = bind(&print);
    fk(h)
}
```
以上没有报任何错误！所以void()类型能转换成void(xxx)类型？amazing!

因为有以上转换，完全可以增加一个新的`typedef function<void(Timestamp)> ReadEventCallback;`
其参数传入poller返回时的时间辍，却不影响之前的所有设计函数，因为需要用Timestamp自然可以用，不需要的继续调用自己的空参数函数就好，
就是因为存在以上可调用对象的转换。


**linux时间处理**

主要参考``std::string Timestamp::toFormattedString()``,输出当前时区非常细致的时间

主要通过gettimeofday() + localtime() + snprintf()

**缓冲区**

之前的实现没有加输入输出缓冲区，而非阻塞I/O的网络程序必须在应用层增加缓冲区，因此开辟了新类Buffer作为缓冲区，其逻辑上的详细说明在书上有，
这里主要谈源码实现的一些特别之处。

感觉最主要的就是栈上Buffer,和readv一次性读完，增加了不少效率。有个细节时是`append()`回去的时候会检查空间大小再append在后面。

`makespace()`函数源码实现有个细节，空间不足时会先检查能不能往前挪，不可以再申请一个更大的内存。

因为对用户接口变成了缓冲区，所以用户想管理自己的数据可能就得详细阅读Buffer的API了

因为感觉没太多好说的，书上写的非常详细，源码也很清晰，是一个设计的很好的Buffer类了

```c++
void onConnection(const TcpConnectionPtr& conn)
{
  if (conn->connected())
  {
    printf("onConnection(): new connection [%s] from %s\n",
           conn->name().c_str(),
           conn->peerAddress().toHostPort().c_str());
  }
  else
  {
    printf("onConnection(): connection [%s] is down\n",
           conn->name().c_str());
  }
}

void onMessage(const TcpConnectionPtr& conn,
               Buffer* buf,
               Timestamp receiveTime)
{
  printf("onMessage(): received %zd bytes from connection [%s] at %s\n",
         buf->readableBytes(),
         conn->name().c_str(),
         receiveTime.toFormattedString().c_str());

  printf("onMessage(): [%s]\n", buf->retrieveAsString().c_str());
}
```
一种比较典型的使用方法,取出数据打印数据，全部丢弃。

**irono是如何确保数据发至对方的**

首先确定的是，对于使用irono的用户，发送过程是透明的，他只需要使用send(string)就可以了，irono会确保将数据完整原封不动的发送过去。

irono是这样确保的：

*非阻塞I/O + Output Buffer + level trigger*, 首先第一次发送时，如果Output Buffer中没有数据囤积，则直接尝试::write系统调用往内核写，成功发送则一切OK，若出现某些问题，
如

1. 内核缓冲区不够，irono会将没发送完的数据append进Output Buffer中，将POLLOUT置位，在下一个循环中会调用handleWrite继续发Output Buffer部分。
2. 发生错误，一般表现在对方关闭了连接，此时irono会同样append进Output Buffer,但是下一次循环时，handleRead会read到0，然后直接关闭连接，最后所有缓冲区析构，所以没有问题。
3. 在handleWrite()期间若对面关闭连接同样不会有事，因为也会在下一个循环的handleRead()中处理。
   
所以在使用的用户看来，除了对面关闭了连接，只要时间足够长，最终数据一定会发出去原封不动的给对方。

**irono回调流程逻辑**

1. 程序开始:

    TcpServer初始化列表中初始化Accpetor, Accpetor构造函数中绑定自己的handleRead()回调

    `acceptChannel_.setReadCallback( bind(&Acceptor::handleRead, this) );`

    TcpServer构造函数体中绑定acceptor的newConnectionCallback回调

    `acceptor_->setNewConnectionCallback(bind(&TcpServer::newConnection, this, _1, _2));`

    用户绑定自己的连接到来时的回调函数onConnection()

    `    void setConnectionCallback(const ConnectionCallback& cb)
        { connectionCallback_ = cb; }`


2. 程序启动:

    TcpServer::start()开启监听

    `loop_->runInLoop(bind(&Acceptor::listen, get_pointer(acceptor_)));`

    Acceptor::listen()中将监听套接字注册到Poller中

    ```c++
    void Acceptor::listen() {
        listenning_ = true;
        acceptChannel_.enableReading();
        acceptSocket_.listen();
    }
    ```

3. 新连接到来
   
   Acceptor的channel会回调函数 *Acceptor::handleRead()*, 在连接accept后转发给

   `newConnectionCallback_(connfd, peerAddr)`对应回调函数*TcpServer::newConnection()*

   在其中会建立新的TcpConnection对象

   `TcpConnectionPtr conn( new TcpConnection(loop_, connName, sockfd, localAddr, peerAddr));`

   接着传递所有用户回调给TcpConnection对象

   ```c++
    conn->setConnectionCallback(connectionCallback_);
    conn->setMessageCallback(messageCallback_);
   ```

   最后执行连接建立完成的函数

    `conn->connectEstablished();`

    于其中会注册连接套接字给Poller,并在此之后最终执行了用户的connectionCallback

    ```c++
    void TcpConnection::connectEstablished() {
        channel_->enableReading();
        connectionCallback_(shared_from_this());
    }
    ```

tips:

   * 用户的connectionCallback_,连接建立时执行一次，死亡时handleClose会再实行一次,生涯期一共两次
   * 用户的onMessage()函数调用逻辑简单，就是read完套接字到inputBuffer_时调用
   * EventLoop.loop()后，Poller才收得到新连接并处理，毕竟还没开始loop
   * 服务器可能要预热一会才能真正开始工作

**发送信息send()**

实现了线程安全，如果用户调用这个函数的地方是自己的I/O线程则直接当场发送就好，如果不是，将其加入I/O线程的回调函数队列，并唤醒对应的I/O线程。

所以最大的不同是，send()函数的调用地点是服务器用户控制的。而Read()的时机却是对面客户端发送过来就要回调了。

write()返回-1大概有以下情况
1. 内核缓冲区满了，首次返回0，再写返回-1,errno置位EAGAIN & EWOULDBLOCK
2. 对方套接字已经强行关闭，即对方发了RST，第一次写这个套接字无事发生，只会在Poller中置位这个套接字为POLLERR和POLLIN。如果你再去连续写这个套接字，errno置位EPIPE,操作系统给进程发SIGPIPE,如果服务器没忽略这个信号，服务器会异常死亡（退出），所以一定要忽略此信号。具体做法搞个全局变量，构造函数忽略这个信号。
3. 补充：如果忽略了SIGPIPE后，就不会触发POLLERR了，换句话说操作系统不会置位POLLERR了。
   
做了一个测试，没关闭SIGPIPE，POLLERR这个信号就算你没置位去观察这个信号，也是可以返回这个信号的，当我向一个关闭的套接字write的时候，第一次可以write并返回了所有write的字节(不过可想而知操作系统丢弃了)，然后操作系统会在这个套接字上置位POLLERR.
其中errno如下:

`2021-12-02 21:33:59 ERROR 261572  TcpConnection::handleError [0.0.0.0:9981#1] - SO_ERROR = 32 Broken pipe -- TcpConnection.cc:196`

~~在我的逻辑中，会在下一个循环输出以上日志,然后在handleRead()回调中read到0然后关闭套接字。~~(可以忽略这句话)



**DEBUG新体验**

一定要写好日志，丰富的日志信息能让你迅速定位到可能发生问题的地方，然后再去找，这时候找不到再用GDB一步步找。

> Nagle算法默认开启

**输出缓冲区高水位回调**(还没实现)

提供了这样一个回调供框架给使用者解决客户端读取速度慢，导致服务器输出缓冲区不断扩充撑爆内存的问题，用户可以自己实现。

提供一个暴力的实现思路，当输出缓冲区大于10MByte时直接丢弃缓冲区。

**再次深入理解one loop per thread**

像我之前所说的，这个方法显著的减少了线程安全的考量，极大的减少了线程锁的使用，使各连接事件并行，基本上没有竞态条件。

例子：

1. 创建连接是在主Reactor创立的TcpConnection对象，但是最后一步连接建立是调用的TcpConnection的函数connectEstablished(),所以把这个函数转给它对应的I/O线程，在它的I/O线程中处理它自己的逻辑，比如这函数会将它自己注册到自己的Poller，这显然不应该其他线程去代做，否则很容易造成线程安全问题。
2. 关闭连接removeConnection()，这个函数更加复杂，因为调用这个函数的线程一定是TcpConnection对象自己的线程，这是在handleClose()中执行的，但是之后要删除总ConnectionMap中对应的TcpConnection，其由主Reactor管理并统领全局，为了线程安全应转发给主Reactor去删除，接着需要删除从Reactor的Poller复用器事件，又重新需要转发给从Reactor，最终才能回收该套接字。

因此**one loop per thread**它就是通过各级I/O线程管理自己的事件，当发生某个事件时，如果发生事件的位置不是应该处理的线程，那么这个线程应该转发给实际处理的I/O线程，各级线程各司其职，并通过转发可回调对象来进行通信。

**EventLoopThread与EventLoopThreadPool**

每个TcpServer都有一个从Reactors池对象，默认池中没有线程，即典型的单线程Reactor模型。每一个从reactor线程都是一个EventLoopThread。

如果设置了Reactors池线程数量，那么TcpServer start()时，会创建从I/O线程，之后每次新连接到来，都会用round robin方式分发，不会造成偏载。

所有都采用智能指针unique_prt管理，不会有内存泄漏问题。

> reators线程池对用户是透明的

**定时器相关类大修**
在增加定时器Timer的取消接口时，发现了很多问题，如：

1. 删除了的定时器的地址，很有可能会被下一个定时器复用，带来不可估计的后果
2. 自注销的情况，比如在该定时器的事件中取消了该定时器会怎样，怎么解决？

对于1：

只能引入Timer类的静态数据成员，用来计数，计当前定时器的序号，按照这个静态计数来给定时器编号，但是因为线程安全性，
又不得不引入lock-free设计，因为线程锁显得太小题大作了（难道为了一个静态变量专门引入一个线程锁的成员吗？），因此我这里copy了muduo的Atomic类，借用了其原子操作。

对于2：

有点复杂，引入了很多数据成员，比如callingExpiredTimers_检测是不是发生了这种情况，如果是的，会被放入一个TimerId的集合(ActiveTimerSet)，
用来记录这个要被删除的成员，会在之后调用reset()中根据记录不会再将这个定时器放进去了。

这个确实不好用语言表达，十分繁琐，自己看源码吧，源码清晰的多。

**Connector类**

这也是一个相当麻烦的类，发起连接比接受连接复杂一些。充斥了一堆的错误处理，还得考虑重试。

发起连接的基本方式是::connect(),实现有一些注意事项：

1. socket一次性，一但出错，只能关闭重来。体现在源码中就是每次重新连接都要换新的socket和channel。
2. 错误代码ERRNO跟accept4()不同，EAGAIN表示真出错了，本机可用端口用完，正在连接是EINPROGRESS,即使套接字可写也要确认。
3. 重试间隔需要增加。Connector析构函数中要注销定时器。
4. 需要及时清除Poller里的，eventloop里的channel,因为应该连接成功后和他们就没关系了，这和服务器是有区别的。
5. 处理自连接。

两个供用户使用的可调参数
```c++
//最大重试间隔
static const int kMaxRetryDelayMs = 30*1000;
//初始重试间隔
static const int kInitRetryDelayMs = 500;
```
**TcpClient类**

目前就是一个非常粗略的实现，建议在单线程下使用，很多线程安全问题还没有解决。

因为本来重心在服务器上，所以这个就当一个测试用。

**异步日志性能测试**

关闭日志内存保护措施(实测开不开都一样).

测试程序：
```c++
///FileUtil.cc也要增加g_total变量，并且磁盘写多少字节增加多少

inline double timeDifference(Timestamp high, Timestamp low)
{
  int64_t diff = high.microSecondsSinceEpoch() - low.microSecondsSinceEpoch();
  return static_cast<double>(diff) / Timestamp::kMicroSecondsPerSecond;
}
extern unsigned long g_total;

void bench(const char* type)
{
  Timestamp start(Timestamp::now());

  int n = 1000*1000;
  const bool kLongLog = true;
  string empty = " ";
  string longStr(3000, 'X');
  longStr += " ";
  for (int i = 0; i < n; ++i)
  {
    LOG_TRACE << "Hello 0123456789" << " abcdefghijklmnopqrstuvwxyz"
             << (kLongLog ? longStr : empty)
             << i;
  }
  Timestamp end(Timestamp::now());
  double seconds = timeDifference(end, start);
  printf("%12s: %f seconds, %lu bytes, %10.2f msg/s, %.2f MiB/s\n",
         type, seconds, g_total, n / seconds, g_total / seconds / (1024 * 1024));
}

int main()
{
  getppid(); // for ltrace and strace

  LOG_TRACE << "trace";
  LOG_DEBUG << "debug";
  LOG_TRACE << "Hello";
  LOG_TRACE << "World";
  LOG_ERROR << "Error";
  LOG_TRACE << sizeof(Logger);
  LOG_TRACE << sizeof(LogStream);
  LOG_TRACE << sizeof(LogStream::Buffer);

  sleep(1);
  bench("nop");
}

```
短日志: 50字节信息 + 日志头尾60字节 总约110字节;  大约1.55us一条信息。

`irono.log: 29.485835 seconds, 2027036599 bytes,  678291.80 msg/s, 65.56 MiB/s`

长日志: 2940字节信息 + 日志头尾60字节 总约3000字节

`irono.log: 3.635880 seconds, 2265893717 bytes,  275036.58 msg/s, 594.33 MiB/s`